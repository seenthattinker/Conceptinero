IMPORTANT: Use existing materials and documents generated during your development work to extract the pertinent information to complete these questions.

242 – What technological obstacles did you have to overcome? 350 words max – 50 lines of 78 characters

The state of technology at the beginning of the project had several shortcomings and/or limitations, as well as technological problems and unknown elements, as described below:

Before deciding to go with 360 video we did many things to help advance the performance. We reduced the amount of real-time lighting effects and experimented with mobile-optimized shaders applied to the materials, which were two things we thought would increase quality, but so minimally that the result was still not useful. Although some shaders are done in Unity, we needed to enhance and extend capability for others to attempt to better performance on our own. 

The hypothesis was to use a 360 video and record the entire thing on a camera set up, mapping the video onto a sphere. Using this we were able to achieve the quality we wanted, and it would still perform well on most phones.
However, there was an issue with Unity engine. It does not support playing videos within an environment natively. It was uncertain how we could develop an approach to determine how we could overcome this limitation, and still satisfy mobile device CPU GPU limitations. There were Apple app store plug-ins to attempt to remedy issues, but those did not work, as it was not able to take the frames from the video and map it onto the sphere as we expected.  




244 – What work did you perform in the tax year to overcome those technological obstacles?   700 words max - 100 lines of 78 characters


At the beginning of this project in 2017, mobile VR was gaining popularity because it didn’t require the use of an overly expensive computer.  If you were to run VR on the Oculus Rift or HTC Vive, it goes through the graphics card. In terms of running VR on just a regular computer, the GPU was the limiting factor. In our project context, we first attempted to render everything in real-time. The number of polygons that could be drawn on the screen at one time, any lighting effects we wanted to use, any real-time lights within the environment, any camera post effects that give the scene a different feel were all impacted when moving to the Mobile. Using the Vive, or using Oculus, we would be able to achieve the above-mentioned effects with using a computer but could not using a mobile device.  

The hypothesis was to use a 360 video and record the entire thing on a camera set up, mapping the video onto a sphere. Using this we hoped to achieve the quality we wanted, and it would still perform well on most phones.
Anything that was going to be consistent in a scene, was recorded. For example, a park could be rendered into the 360 videos because it was consistent as the entire scene took place at this one setting.  Inconsistent pieces can still be rendered in the scene. Someone riding their bike during this scene looked at their cell phone and a menu was brought up. This could be accessed each time and was inconsistent but was still rendered in the scene. 3D geometry was used for the bike and the hands while everything else was mapped to the video on the sphere. Because of the camera perspective, you would never know it. However, we realized there was an issue with Unity engine: it does not support playing videos within an environment natively. Partial 360 and partial rendering was beginning to become more popular, but at the start of this project it was still evolving. We realized that videos would not play natively so the frames needed mapping. Unity eventually went on to implement something called a movie texture; however, when we were building this, movie textures did not exist and were not available. At the time we developed a plug-in to be able to texture and time images and colors were required. In achieving this, we would use this same plug-in for future implementations. 

In our GI context, there are six different diffused conditions which are applied to the entire segment of the GI. For the transverse colon for example, you can have three or four of these different conditions. Textures should not be combined, it will begin to slow down. We couldn’t use a 360 video for this. All our customized textures were painted and developed in Z-brush. But we found that the layers could not be exported and placed on top of one another. We needed to process that required development of a script of all the conditions that could be applied to that section and export the normal map and texture map from there into Unity. We could then patch that through to the app to apply the correct texture. We realized that it is not the most efficient way but was the best at the time. The script runs through and turn layers on and off allowing any combinations to be created. The result was 256 different textures. These textures were reviewed afterward to see if they matched what we wanted. A code was given to each, so we could map which segment of the GI it was assigned. We coded a binary string based on if that condition is on or off, and when the QR code is scanned by the app the information is taken from that server, and for the region we can see which conditions are applied at that time. We developed this process such that, for any paint on any texture, we can read the string and calculate which one to apply, and which geometries. 

Next, we needed to focus on displaying the prescribing information at any point any time in the viewable area. There was no way in Unity to display a PDF containing the source content, so needed to develop an extension approach ourselves. To allow the prescribing information at any given point in time, we hypothesized and developed a custom plug-in that could displays a WebView in the VR viewing area showing the PDF. Although this useful in our prototype, it was not perfect from all aspects of the VR space. The concept would be kept for internal uses, as the WebView/PDF plug-in approach will be useful for other issues in the future. 

By fiscal year end, we had this iPhone prototype working, but would continue work for other devices in the next fiscal period.  

246 – What technological advancements were you trying to achieve? 	350 words max - 50 lines of 78 characters

The technological objective of the project is development of interactive texture library for dynamic color and texture data which is generated external to Unity but mapped at run-time.  

The genesis of this project occurred after due diligence to attempt to achieve the following: After a doctor has preformed a colonoscopy they can graphically map all their findings onto a Gastrointestinal tract (GI) on their browser. They can then export a QR code which the patient can scan and ‘see’ their GI in VR. This child safety view is a way for the patient to better see and understand what is happening in their body. Despite this new information being presented in a much more creative, child friendly manner; it was neither cost-effective, nor technologically possible with existing technologies. 
 
In terms of Experiential VR versus Narrative VR, the biggest issue with any mobile VR is having the best quality while knowing the limitations of using a phone; a phone will not produce as high-quality as a computer as there is less GPU, CPU, and processing power; therefore, visuals will not appear perfectly as expected, failing the Virtual ‘Reality’ expectation. We had a goal for how it should look and what we wanted it to look like when we put it on a phone. Once it got on the phone, objectives were not met, and the quality had to be downscaled, which was unacceptable. 

The current state of technology in game engines and hardware input device did not allow, and were not built to apply WebViews, color, and realistic textures correctly on mobile devices, as when we attempted to integrate native applications of the above within the game engine, the expected results outcome of the technologies could not be achieved natively, and the system failed.  

The work done resulted in the advancement our knowledge of information technology, and specifically in development of WebView, color, and realistic texture rendering for virtual reality systems. The content layering system which was developed comprises a collection of WebViews and plug-ins which could be viewed in the VR space. It was a large knowledge advancement to find out how we could provide realistic color texture mapping through our experimentation so that the engine could interpret input better and provide more realistic object representations, such as WebView, colors and textures, with reduced mobile device CPU and GPU power.  


