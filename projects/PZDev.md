Patient News - PZ Development SR&ED - 2019-2020
==============================================================



Integration with embedded Tableau reports and dealing with authentication
-------------------------------------------------------------------------------------

We needed to integrate our application with Tableau.  The requirements were to allow users to directly see Tableau reports ( charts / data tables ) into the MPD (Patient News Marketing Application)  dashboard without needing to login to the Tableau server.
The difficulties were related to allowing the system to access the Tableau server without the users having to input a username and a password. We had difficulties with this because of the communication between the ruby on rails framework ( used to create the application ) and the Tableau server. The integration needed to enable SAML single sign-on feature and since we didn’t find a ruby gem ( plugin ) that would manage the connection between tableau and a rails application, we needed to test some other implementations where we would directly use the SAML protocol to call the tableau servers. 

We tried to use the following plugin ( https://github.com/civisanalytics/tableau_api ) but it allowed to connect to the Tableau server API and query the data. We wanted to directly display some charts generated by Tableau. We wanted to embed this generated code into our application. It was working, but the users needed to login to the Tableau service ( with the Tableau credentials ) in order to actually see the charts. So, our requirement was to implement a single sign on functionality that would allow to users from our application to only login and directly see the Tableau data.
Since the plugin above was not suitable for our needs,  our approach was to create a connection between our Ruby on Rails application to the Tableau server using the SAML protocol. The implementation was not finished as we only did research on how to use the SAML protocol and the Single-Sign-On functionality of Tableau. What we learned from this approach was actually more related to the configurations on the Tableau server side that would allow the SAML connection of the users. Our failed attempts are mostly related to SAML configurations.
We finally made it work using a recent functionality that Tableau developed called Trusted Authentication ( https://help.tableau.com/current/server/en-us/trusted_auth.htm ).
This is basic in terms of API implementation but the whole process to get to this one required a lot of trial and errors.


Implement a method to automatically determine the missed opportunity calls and the no score calls.
There’s a third part company called Voicebase that is actually doing the AI on this but we were also involved in this:
We analyzed the calls to see their specific details for missed opportunities.
We developed some scripts to regularly train the voicebase AI ( submit recordings to their API in order marked with answers from our scoring team).


System performance improvements
---------------------------------------

We experienced performance issues of the system because of the large datasets used and also because of the heavy importing jobs that were running in parallel with the application normal usage so the database server was very loaded and the queries performed were very slow. (this was still happening after the scheduling method developed above in March). End user performance was very slow, the UI was lagging. 
	We tried a lot of approaches such as:
Using materialized views to load the data and save it in the needed format ( as flat tables ). This way, the queries were simpler and without complex joins. The issue we experienced here was that these views needed to be refreshed from time to time so the user will always have the latest data. The refresh of a single view was taking a lot of time and in the meantime we didn’t have the guarantee that we’re showing recent / reliable data to the users.
Using the latest features of PostgreSql database to speed up the queries ( ex: filters )
Using filters to speed up the queries. Tried using existing tools in the db to speed up the queries. This allowed the queries to select the data faster, replaces “where” statements and retrieves it faster. But this wasn’t fast enough. Measured the total load time of the page. Page load time reduced from 30-40 seconds to 20-25 seconds (approx). Exhausted all the db features and now had to develop custom methods.
Optimizing the queries to retrieve and lookup the optimum amount of data
Large queries looking up data from 4 to 5 tables. Experimented with moving conditions between queries to optimize the query structure (joins etc) and see if that sped up the searches. The page would now load in 10-15 seconds. All the queries were triggered at the same time, and when they were all ready the data was displayed.
Incremental steps here:
Add indexes on all relevant fields used in queries ( in conditions ). This was a very good improvement especially for large tables.
In case of joins, move conditions form the where clause to the join on clause in order to restrict the number of rows returned by the query .
In case of inserts into the database, use multiple rows inserts. So, don’t do a single insert statement per row, but add multiple rows at once. We learned that this is improving the general database performance as it don’t stay too much on inserts and updates and can work more on the queries. 
Remove unnecessary columns from the SELECT statement
Avoid using Order By when not necessary

Loading the data in the application asynchronously so the users will not have to wait for the whole page to load, but instead they will see data as it’s coming from the database.
Display page in 2-3 seconds, and then display the data as it is loaded. THis made the page loading feel faster. Split the page into different sections, for each section did a request to the server, when the data was ready it was displayed. Now they call the queries as needed and display the data as they get it. Parallel requests sent to the server, data displayed as processing ends. They don’t wait for all the requests to finish.
We managed to improve the system using the last 2 options experienced above.

Salesforce to PrintVis integration
-----------------------------------------------

The company migrated the operations from Salesforce to PrintVis (Microsoft Dynamics Nav) so an ETL(Extract, Transform, Load) tool was needed to be developed in order to migrate and sync the data.
	The challenge was that there was not an existing extractor between the 2 systems and we had to do a custom tool to accommodate the company’s data and the 2 systems data structure. We used APIs (Salesforce / PrintVis) to get / insert the data from one system to another but we had a series of items to be taken into considerations such as:
Data sync (make sure that we don’t have duplicates and the same data is in both systems) - We had to do some trial & error steps here in order to make sure that the data structure is correctly achieved in both systems
Performance - The sync need to be reasonably fast for the large amount of the data to import so we had some trial and error steps as well including optimizing queries and also optimizing the read / write of the data
\


Integration between 2 systems ( ruby and angular )
------------------------------------------------------------

	We are working into transitioning the system from ruby on rails to a Node js / Angular js microservices system.
	Since this is a long project, we had requirements to integrate Angular modules one by one once they are ready. 
	This means that a logged in user should seamlessly be able to go from a ruby page to an angular page without even noticing that he’s on another application.
	Here, we had some trial and error approach until we managed to be successfully. 
	A regular approach was to duplicate the login functions and call the login methods for both system once or call the login method every time the user would go from ruby to angular.
However, this was not acceptable at both user experience and system performance.
	So the final version of this consisted in using some domain shared cookies between the 2 systems that would hold the state of the logged in user. Basically, when the user logs in in ruby, we also call the node js endpoint to log him in and save the token in a cookie. Then, every time an angular page is accessed, it would search for these cookies and authorize or deny the request.
